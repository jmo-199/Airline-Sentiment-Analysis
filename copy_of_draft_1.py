# -*- coding: utf-8 -*-
"""Copy_of_Draft_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fCiZY1qWkywxyjSpESkxJirs31MQrHBh
"""

!pip install nltk
!pip install mlxtend

#importing req. Lib.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from mlxtend.plotting import plot_confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

#load our data set
data = pd.read_csv('/content/drive/MyDrive/CC project/Tweets.csv')
data.shape

from google.colab import drive
drive.mount('/content/drive')

data.head()

data.tail()

data.info()

#checking unique values
data.nunique()

#checking null values in our data
data.isnull().sum()

"""# Preprocessing the Data"""

#tweet_created column got the date recorts and showing type is object we have to change it of date time format
data['tweet_created'] = pd.to_datetime(data['tweet_created']).dt.date
data['tweet_created'] = pd.to_datetime(data['tweet_created'])
data.info()

data.head()

#checking uniques values in tweet_created columns
data['tweet_created'].nunique()

numberoftweets = data.groupby('tweet_created').size()
numberoftweets

"""## Dealing with Null Values"""

print("Percentage null or na values in df")
((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)

"""#### airline_sentiment_gold, negativereason_gold have more than 99% missing data And tweet_coord have nearly 93% missing data. It will be better to delete these columns as they will not provide any constructive information"""

del data['tweet_coord']
del data['airline_sentiment_gold']
del data['negativereason_gold']



freq = data.groupby('negativereason').size()
freq



"""# Exploratory Data Analysis"""

counter = data.airline_sentiment.value_counts()
index = [1,2,3]
plt.figure(1,figsize=(12,6))
plt.bar(index,counter,color=['purple','orange','green'])
plt.xticks(index,['Negative','Neutral','Positive'],rotation=0)
plt.xlabel('Sentiment Type')
plt.ylabel('Sentiment Count')
plt.title('Count of Type of Sentiment')

"""#### Sentiment by Airlines"""

print("Total number of tweets for each airline \n ",data.groupby('airline')['airline_sentiment'].count().sort_values(ascending=False))
airlines= ['US Airways','United','American','Southwest','Delta','Virgin America']
plt.figure(1,figsize=(12, 12))
for i in airlines:
    indices= airlines.index(i)
    plt.subplot(2,3,indices+1)
    new_df=data[data['airline']==i]
    count=new_df['airline_sentiment'].value_counts()
    Index = [1,2,3]
    plt.bar(Index,count, color=['red', 'purple', 'green'])
    plt.xticks(Index,['negative','neutral','positive'])
    plt.ylabel('Mood Count')
    plt.xlabel('Mood')
    plt.title('Count of Moods of '+i)



"""#### Percentage of the negative reviews for each airline."""

neg_tweets = data.groupby(['airline','airline_sentiment']).count().iloc[:,0]
total_tweets = data.groupby(['airline'])['airline_sentiment'].count()

my_dict = {'American':neg_tweets[0] / total_tweets[0],'Delta':neg_tweets[3] / total_tweets[1],'Southwest': neg_tweets[6] / total_tweets[2],
'US Airways': neg_tweets[9] / total_tweets[3],'United': neg_tweets[12] / total_tweets[4],'Virgin': neg_tweets[15] / total_tweets[5]}
perc = pd.DataFrame.from_dict(my_dict, orient = 'index')
perc.columns = ['Percent Negative']
print(perc)

# List of colors for each airline
colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange']

# Plotting with unique colors for each airline
ax = perc.plot(kind='bar', rot=0, color=colors, figsize=(15,6))
ax.set_xlabel('Airlines')
ax.set_ylabel('Percentage of negative tweets')
plt.show()



figure_2 = data.groupby(['airline', 'airline_sentiment']).size()
figure_2.unstack().plot(kind='bar', stacked=True, figsize=(15,10))



"""#### Top 10 Reasons behing Negative Emotion"""

negative_reasons = data.groupby('airline')['negativereason'].value_counts(ascending=True)
negative_reasons.groupby(['airline','negativereason']).sum().unstack().plot(kind='bar',figsize=(22,12))
plt.xlabel('Airline Company')
plt.ylabel('Number of Negative reasons')
plt.title("The number of the count of negative reasons for airlines")
plt.show()



"""#### Negative emotion for Each of the Airlines"""

#get the number of negative reasons
data['negativereason'].nunique()

NR_Count=dict(data['negativereason'].value_counts(sort=False))
def NR_Count(Airline):
    if Airline=='All':
        a=data
    else:
        a=data[data['airline']==Airline]
    count=dict(a['negativereason'].value_counts())
    Unique_reason=list(data['negativereason'].unique())
    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']
    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})
    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])
    return Reason_frame
def plot_reason(Airline):

    a=NR_Count(Airline)
    count=a['count']
    Index = range(1,(len(a)+1))
    plt.bar(Index,count, color=['red','yellow','blue','green','black','brown','gray','cyan','purple','orange'])
    plt.xticks(Index,a['Reasons'],rotation=90)
    plt.ylabel('Count')
    plt.xlabel('Reason')
    plt.title('Count of Reasons for '+Airline)

plot_reason('All')
plt.figure(2,figsize=(13, 13))
for i in airlines:
    indices= airlines.index(i)
    plt.subplot(2,3,indices+1)
    plt.subplots_adjust(hspace=0.9)
    plot_reason(i)

"""#### Word Cloud for Negative sentiment"""



"""#### Droping the rows with neutral sentiments"""

data.drop(data.loc[data['airline_sentiment']=='neutral'].index, inplace=True)

"""#### Label Encoding"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(data['airline_sentiment'])

data['airline_sentiment_encoded'] = le.transform(data['airline_sentiment'])
data.head()

"""### Preprocessing the tweet text data"""

# cleaning the tweet text data to apply classification algorithms on it

import re

def tweet_to_words(tweet):
    letters_only = re.sub("[^a-zA-Z]", " ", tweet)
    words = letters_only.lower().split()
    stops = set(stopwords.words("english"))
    meaningful_words = [w for w in words if not w in stops]
    return " ".join(meaningful_words)

from nltk.corpus import stopwords

nltk.download('stopwords')
data['clean_tweet'] = data['text'].apply(lambda x: tweet_to_words(x))

### Vectorization
x = data.clean_tweet
y = data.airline_sentiment

print(len(x), len(y))

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=0.25)
print(len(x_train), len(y_train))
print(len(x_test), len(y_test))

from sklearn.feature_extraction.text import CountVectorizer

# instantiate the vectorizer
vect = CountVectorizer()
vect.fit(x_train)

# Use the trained to create a document-term matrix from train and test sets
x_train_dtm = vect.transform(x_train)
x_test_dtm = vect.transform(x_test)

vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)
vect_tunned

"""## Model Building

### 1. SVM
"""

#training SVM model with linear kernel
#Support Vector Classification-wrapper around SVM
from sklearn.svm import SVC
model = SVC(kernel='linear', random_state = 10)
model.fit(x_train_dtm, y_train)
#predicting output for test data
pred = model.predict(x_test_dtm)

#accuracy score
accuracy_score(y_test,pred)

#building confusion matrix
cm = confusion_matrix(y_test, pred)
#print(cm)


#confusion matrix to DataFrame
conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1',], index = ['Actual:0','Actual:1',])
#plotting the confusion matrix
sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = 'Paired', cbar = False,linewidths = 0.1, annot_kws = {'size':25})
plt.xticks(fontsize = 20)
plt.yticks(fontsize = 20)
plt.show()

print(classification_report(y_test,pred))

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
!tar xf spark-3.2.0-bin-hadoop3.2.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "spark-3.2.0-bin-hadoop3.2"

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
sc = spark.sparkContext
sc

from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.sql.functions import col

data2 = spark.read.csv('/content/drive/MyDrive/CC project/Tweets.csv', header=True, inferSchema=True)

data2 = data2.dropna(subset=['text'])

from pyspark.sql.functions import col, when

# Assuming your DataFrame has 'airline_sentiment' as the sentiment label
# You might need to adjust these values based on your actual data
data2 = data2.withColumn(
    'label',
    when(col('airline_sentiment') == 'positive', 0)
    .when(col('airline_sentiment') == 'negative', 1)
    .when(col('airline_sentiment') == 'neutral', 2)
    .cast('double')
)
data2.show()

(trainingData, testData) = data2.randomSplit([0.8, 0.2], seed=42)

# Configure an ML pipeline with three stages: tokenizer, hashingTF, and logistic regression
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol="idf_features")
lr = LogisticRegression(featuresCol="idf_features", labelCol="label", maxIter=10)

pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])

model = pipeline.fit(trainingData)

# Make predictions on the test set
predictions = model.transform(testData)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Evaluate the model using a MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# Print the accuracy
print("Accuracy: {}".format(accuracy))

import seaborn as sns
import matplotlib.pyplot as plt
from pyspark.mllib.evaluation import MulticlassMetrics

# Assuming you have 'predictions' DataFrame with 'label' and 'prediction' columns
predictionAndLabel = predictions.select("prediction", "label").rdd.map(lambda row: (float(row["prediction"]), float(row["label"])))

# Instantiate the MulticlassMetrics class with the RDD
metrics = MulticlassMetrics(predictionAndLabel)

# Get the confusion matrix
confusion_matrix = metrics.confusionMatrix().toArray()

# Convert the confusion matrix to a Pandas DataFrame
conf_matrix = pd.DataFrame(confusion_matrix, columns=['Predicted:0', 'Predicted:1', 'Predicted:2'], index=['Actual:0', 'Actual:1', 'Actual:2'])

# Plot the confusion matrix using seaborn
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Paired', cbar=False, linewidths=0.1, annot_kws={'size': 15})
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Assuming you have 'predictions' DataFrame with 'label' and 'prediction' columns
# Replace 'predictions' with the actual name of your DataFrame variable
predictions.createOrReplaceTempView("predictions")

# Precision, Recall, and F1 Score for each class
class_metrics = spark.sql("""
    SELECT
        label,
        ROUND(SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) / SUM(CASE WHEN prediction = label THEN 1 ELSE 0 END), 2) AS precision,
        ROUND(SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) / SUM(CASE WHEN label = label THEN 1 ELSE 0 END), 2) AS recall,
        ROUND(2 * (SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) / SUM(CASE WHEN prediction = label THEN 1 ELSE 0 END)) * (SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) / SUM(CASE WHEN label = label THEN 1 ELSE 0 END)) / ((SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) / SUM(CASE WHEN prediction = label THEN 1 ELSE 0 END)) + (SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) / SUM(CASE WHEN label = label THEN 1 ELSE 0 END))), 2) AS f1_score
    FROM predictions
    GROUP BY label
""").toPandas()

# Print class-specific metrics
print(class_metrics)

"""The sentiment analysis results for US airline flights, as indicated by the provided class-specific metrics, offer insights into customer sentiments and satisfaction. Let's interpret these findings in the context of flights and customer experiences:

1. **Positive Sentiment (label 0):**
   - **Precision:** 1.0 (100%)
   - **Recall:** 0.65 (65%)
   - **F1 Score:** 0.79 (79%)

   Interpretation:
   - Customers expressing positive sentiments are accurately identified with high precision.
   - However, the model misses a significant portion of positive sentiments, suggesting that there might be positive experiences that are not well-captured.
   - The F1 score indicates a moderate balance between precision and recall for positive sentiments.

   Implications:
   - While the model is effective in identifying positive sentiments, there is room for improvement in capturing a broader range of positive experiences reported by customers.

2. **Negative Sentiment (label 1):**
   - **Precision:** 1.0 (100%)
   - **Recall:** 0.81 (81%)
   - **F1 Score:** 0.90 (90%)

   Interpretation:
   - The model excels in accurately identifying and classifying negative sentiments, with both high precision and recall.
   - Negative experiences are well-captured, and the model demonstrates a balanced performance with a high F1 score.

   Implications:
   - The model effectively identifies and flags negative sentiments, providing valuable insights into areas of customer dissatisfaction or concerns.

3. **Neutral Sentiment (label 2):**
   - **Precision:** 1.0 (100%)
   - **Recall:** 0.53 (53%)
   - **F1 Score:** 0.69 (69%)

   Interpretation:
   - Neutral sentiments are accurately predicted with high precision, indicating that when the model identifies a neutral sentiment, it is likely correct.
   - However, the recall for neutral sentiments is lower, suggesting that some neutral experiences might be overlooked.

   Implications:
   - The model may miss a portion of neutral customer sentiments, potentially including moderate or neutral experiences that could be relevant for a comprehensive understanding of customer feedback.

**Overall Implications for Flights and Customer Satisfaction:**
- The model performs well in detecting negative sentiments, which is crucial for identifying areas of concern or dissatisfaction among customers.
- Positive sentiments are identified with high precision, but there is room for improvement in capturing a broader range of positive experiences.
- The model's performance for neutral sentiments indicates the need for further refinement to capture a more comprehensive spectrum of customer feedback.

These insights can guide airlines in refining their customer satisfaction strategies, addressing specific areas of concern, and enhancing the overall passenger experience. Additionally, feedback from neutral sentiments may highlight areas where customer experiences can be fine-tuned for greater satisfaction.

"""



















